{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ace494b",
   "metadata": {},
   "source": [
    "# 作业一：实现N-gram语言模型平滑算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d9726",
   "metadata": {},
   "source": [
    "### 预处理\n",
    "\n",
    "首先创建一些预处理函数。\n",
    "\n",
    "引入必要的模块，定义些类型别名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0097797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "Sentence = List[str]\n",
    "IntSentence = List[int]\n",
    "\n",
    "Corpus = List[Sentence]\n",
    "IntCorpus = List[IntSentence]\n",
    "\n",
    "Gram = Tuple[int]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c02038",
   "metadata": {},
   "source": [
    "下面的函数用于将文本正则化并词元化。该函数会将所有英文文本转为小写，去除文本中所有的标点，简单起见将所有连续的数字用一个`N`代替，将形如`let's`的词组拆分为`let`和`'s`两个词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd05065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_splitor_pattern = re.compile(r\"[^a-zA-Z']+|(?=')\")\n",
    "_digit_pattern = re.compile(r\"\\d+\")\n",
    "def normaltokenize(corpus: List[str]) -> Corpus:\n",
    "    \"\"\"\n",
    "    Normalizes and tokenizes the sentences in `corpus`. Turns the letters into\n",
    "    lower case and removes all the non-alphadigit characters and splits the\n",
    "    sentence into words and added BOS and EOS marks.\n",
    "\n",
    "    Args:\n",
    "        corpus - list of str\n",
    "\n",
    "    Return:\n",
    "        list of list of str where each inner list of str represents the word\n",
    "          sequence in a sentence from the original sentence list\n",
    "    \"\"\"\n",
    "\n",
    "    tokeneds = [ [\"<s>\"]\n",
    "               + list(\n",
    "                   filter(lambda tkn: len(tkn)>0,\n",
    "                       _splitor_pattern.split(\n",
    "                           _digit_pattern.sub(\"N\", stc.lower()))))\n",
    "               + [\"</s>\"]\n",
    "                    for stc in corpus\n",
    "               ]\n",
    "    return tokeneds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2768c1",
   "metadata": {},
   "source": [
    "接下来定义两个函数用来从训练语料中构建词表，并将句子中的单词从字符串表示转为整数索引表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4685897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocabulary(corpus: Corpus) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Extracts the vocabulary from `corpus` and returns it as a mapping from the\n",
    "    word to index. The words will be sorted by the codepoint value.\n",
    "\n",
    "    Args:\n",
    "        corpus - list of list of str\n",
    "\n",
    "    Return:\n",
    "        dict like {str: int}\n",
    "    \"\"\"\n",
    "\n",
    "    vocabulary = set(itertools.chain.from_iterable(corpus))\n",
    "    vocabulary = dict(\n",
    "            map(lambda itm: (itm[1], itm[0]),\n",
    "                enumerate(\n",
    "                    sorted(vocabulary))))\n",
    "    return vocabulary\n",
    "\n",
    "def words_to_indices(vocabulary: Dict[str, int], sentence: Sentence) -> IntSentence:\n",
    "    \"\"\"\n",
    "    Convert sentence in words to sentence in word indices.\n",
    "\n",
    "    Args:\n",
    "        vocabulary - dict like {str: int}\n",
    "        sentence - list of str\n",
    "\n",
    "    Return:\n",
    "        list of int\n",
    "    \"\"\"\n",
    "\n",
    "    return list(map(lambda tkn: vocabulary.get(tkn, len(vocabulary)), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265aba2b",
   "metadata": {},
   "source": [
    "接下来读入训练数据，将数据预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19af69f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/news.2007.en.shuffled.deduped.train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\UNIV\\自然语言处理\\Labs\\lab1\\ngram.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/UNIV/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Labs/lab1/ngram.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfunctools\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/UNIV/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Labs/lab1/ngram.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mdata/news.2007.en.shuffled.deduped.train\u001b[39;49m\u001b[39m\"\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/UNIV/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Labs/lab1/ngram.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     texts \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m l: l\u001b[39m.\u001b[39mstrip(), f\u001b[39m.\u001b[39mreadlines()))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/UNIV/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Labs/lab1/ngram.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoaded training set.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/news.2007.en.shuffled.deduped.train'"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "with open(\"data/news.2007.en.shuffled.deduped.train\", encoding=\"utf-8\") as f:\n",
    "    texts = list(map(lambda l: l.strip(), f.readlines()))\n",
    "\n",
    "print(\"Loaded training set.\")\n",
    "\n",
    "corpus = normaltokenize(texts)\n",
    "vocabulary = extract_vocabulary(corpus)\n",
    "corpus = list(\n",
    "        map(functools.partial(words_to_indices, vocabulary),\n",
    "            corpus))\n",
    "\n",
    "print(\"Preprocessed training set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a007b899",
   "metadata": {},
   "source": [
    "### 设计模型\n",
    "\n",
    "参照公式\n",
    "\n",
    "$$\n",
    "P(w_k | W_{k-n+1}^{k-1}) = \\begin{cases}\n",
    "    d(W_{k-n+1}^k) * \\dfrac{C(W_{k-n+1}^k)}{C(W_{k-n+1}^{k-1})} &  C(W_{k-n+1}^k) > 0 \\\\\n",
    "    \\alpha(W_{k-n+1}^{k-1}) * P(w_k | W_{k-n+2}^{k-1}) &  \\text{否则} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "实现N-gram语言模型及采用带Good-Turing折扣的Katz回退算法。\n",
    "\n",
    "需要实现的功能包括：\n",
    "\n",
    "1. 统计各词组（gram）在训练语料中的频数\n",
    "2. 计算同频词组个数$N_r$\n",
    "3. 计算$d(W_{k-n+1}^k)$\n",
    "4. 计算$\\alpha(W_{k-n+1}^{k-1})$\n",
    "5. 根据公式计算回退概率\n",
    "6. 计算概率对数与困惑度（PPL）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea6708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class NGramModel:\n",
    "    def __init__(self, vocab_size: int, n: int = 4):\n",
    "        \"\"\"\n",
    "        Constructs `n`-gram model with a `vocab_size`-size vocabulary.\n",
    "\n",
    "        Args:\n",
    "            vocab_size - int\n",
    "            n - int\n",
    "        \"\"\"\n",
    "\n",
    "        # 字典的大小\n",
    "        self.vocab_size: int = vocab_size\n",
    "\n",
    "        # 表示该语言模型n-gram的取值\n",
    "        self.n: int = n\n",
    "\n",
    "        # 记录语料库中，某个gram的出现的频次\n",
    "        # 例如 self.frequencies[1][(12, 23)] 应等于 (12, 23) 这个 2-gram 在语料库中出现的次数\n",
    "        # 注：第一个下标表示长度，调用时下标要减一\n",
    "        self.frequencies: List[Dict[Gram, int]] = [{} for _ in range(n)]\n",
    "\n",
    "        # 缓存语言模型提供的概率值，即对于某个gram而言 P(`gram[-1]`|`gram[:-1]`)的值\n",
    "        # 例如 self.disfrequencies[2][(12, 23, 77)] 应等于 P(`77`|`12, 23`) 的值\n",
    "        # 注：第一个下标表示长度，调用时下标要减一\n",
    "        self.disfrequencies: List[Dict[Gram, float]] = [{} for _ in range(n)]\n",
    "\n",
    "        # 记录number of count，即在前缀是某个(N-1)-gram的情况下，语料库中出现次数是r次的N-gram的数量\n",
    "        # 例如 self.ncounts[((12, 23)][7] 应等于 语料库中以(12, 23)为前缀的3-gram中，出现次数为7次的种类数\n",
    "        self.ncounts= [{} for _ in range(n)]\n",
    "\n",
    "        # 说明文档中所提到的折扣阈值θ\n",
    "        self.discount_threshold:int = 7\n",
    "\n",
    "        # 对于某个gram而言的插值系数lambda，以 (lambda, 1-lambda) 形式保存\n",
    "        # 例如 self._d[(122, 323)] 应等于 (0.1, 0.9)，（如果lambda=0.1的话）\n",
    "        # 注：这里虽然写作d，但其实应该是公式里的lambda\n",
    "        self._d: Dict[Gram, Tuple[float, float]] = {}\n",
    "\n",
    "        # 缓存对于某个gram而言的回退系数α\n",
    "        # 例如 self._alpha[2][(122, 323)] 应等于α(`122, 323`)\n",
    "        # 注意：调用时，第一个下标虽然表示长度，但不需要减一\n",
    "        self._alpha: List[Dict[Gram, float]] = [{} for _ in range(n)]\n",
    "\n",
    "        # 当计算结果为0时，应取该值，避免潜在的除零错误\n",
    "        self.eps = 1e-10\n",
    "\n",
    "        self.grams = [{}]\n",
    "\n",
    "        self.frq = 0\n",
    "\n",
    "    def learn(self, corpus: IntCorpus):\n",
    "        \"\"\"\n",
    "        Learns the parameters of the n-gram model.\n",
    "\n",
    "        Args:\n",
    "            corpus - list of list of int\n",
    "        \"\"\"\n",
    "\n",
    "        for stc in corpus:\n",
    "            for i in range(1, len(stc)+1):\n",
    "                for j in range(min(i, self.n)):\n",
    "                    # TODO: count the frequencies of the grams\n",
    "                    tp = tuple(stc[i-1-j:i])\n",
    "                    self.frequencies[j][tp] = self.frequencies[j].get(tp, 0) + 1\n",
    "\n",
    "        for i in range(1, self.n):\n",
    "            self.grams.append(dict(itertools.groupby(\n",
    "                    sorted(\n",
    "                        sorted(\n",
    "                            map(lambda itm: (itm[0][:-1], itm[1], itm[0]),\n",
    "                                 self.frequencies[i].items()),\n",
    "                            key=(lambda itm: itm[1])),\n",
    "                        key=(lambda itm: itm[0])\n",
    "                        ), key=lambda itm: itm[0])))\n",
    "\n",
    "        for i, x in enumerate(self.frequencies):\n",
    "            for _, num in x.items():\n",
    "                self.ncounts[i][num] = self.ncounts[i].get(num, 0) + 1\n",
    "\n",
    "        self.frq = float(sum([item[1] for item in self.frequencies[0].items()]))\n",
    "\n",
    "    def d(self, gram: Gram) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the interpolation coefficient.\n",
    "\n",
    "        Args:\n",
    "            gram - tuple of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "        n = len(gram) - 1\n",
    "        if gram not in self._d:\n",
    "            # 对于某个gram而言的插值系数lambda，返回元组 (lambda, 1-lambda)\n",
    "            # TODO: calculates the value of $d'$\n",
    "            numerator1 = self.ncounts[n][1]\n",
    "            numerator2 = (self.discount_threshold + 1) * self.ncounts[n][self.discount_threshold + 1]\n",
    "            denominator = numerator1 - numerator2\n",
    "            self._d[gram] = (numerator1/denominator, - numerator2/denominator)\n",
    "        return self._d[gram]\n",
    "\n",
    "    def alpha(self, gram: Gram) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the back-off weight alpha(`gram`)\n",
    "\n",
    "        Args:\n",
    "            gram - tuple of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        n = len(gram)\n",
    "        if gram not in self._alpha[n]:\n",
    "            if gram in self.frequencies[n-1]:\n",
    "                # 按照定义计算分子分母的 1 - sum (...) 的值即可，\n",
    "                # 提示：式中的P(xxx|xxx)调用下面的\"__getitem__\"函数即可，无需在此单独展开。此函数调用方式为 self[gram]\n",
    "                # 记得考虑分母为0应该怎么处理\n",
    "                # TODO: calculates the value of $\\alpha$\n",
    "                numerator = 1 - sum(self[x[2]] for x in self.grams[n].get(gram, []))\n",
    "                numerator = numerator if numerator else self.eps\n",
    "                denominator = 1 - sum(self[x[2]] for x in self.grams[n-1].get(gram[1:], []))\n",
    "                denominator = denominator if denominator else self.eps\n",
    "                self._alpha[n][gram] = numerator / denominator\n",
    "            else:\n",
    "                self._alpha[n][gram] = 1.\n",
    "        return self._alpha[n][gram]\n",
    "\n",
    "    def __getitem__(self, gram: Gram) -> float:\n",
    "        \"\"\"\n",
    "        Calculates smoothed conditional probability P(`gram[-1]`|`gram[:-1]`).\n",
    "\n",
    "        Args:\n",
    "            gram - tuple of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        n = len(gram)-1\n",
    "        if gram not in self.disfrequencies[n]:\n",
    "            if n > 0:\n",
    "                # 按照P的公式，分类讨论count(gram)>0和count(gram)=0的情况\n",
    "                # TODO: calculates the smoothed probability value according to the formulae\n",
    "                count = self.frequencies[n].get(gram, 0)\n",
    "                if count > 0:\n",
    "                    if count > self.discount_threshold:\n",
    "                        d = 1\n",
    "                    else:\n",
    "                        lambda_plus, lambda_minus = self.d(gram)\n",
    "                        r = self.frequencies[n][gram]\n",
    "                        N_r1 = self.ncounts[n].get(r+1, 0)\n",
    "                        N_r1 = N_r1 if N_r1 else self.eps\n",
    "                        N_r = self.ncounts[n].get(r, 0)\n",
    "                        N_r = N_r if N_r else self.eps\n",
    "                        d = lambda_plus * (r + 1) * N_r1 / r / N_r + lambda_minus\n",
    "\n",
    "                    self.disfrequencies[n][gram] = d * self.frequencies[n].get(gram, self.eps) / self.frequencies[n-1].get(gram[:-1], self.eps)\n",
    "                else:\n",
    "                    self.disfrequencies[n][gram] = self.alpha(gram[:-1]) * self[gram[1:]]\n",
    "            else:\n",
    "                self.disfrequencies[n][gram] = self.frequencies[n].get(gram, self.eps) / self.frq\n",
    "        return self.disfrequencies[n][gram]\n",
    "\n",
    "    def log_prob(self, sentence: IntSentence) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the log probability of the given sentence. Assumes that the\n",
    "        first token is always \"<s>\".\n",
    "\n",
    "        Args:\n",
    "            sentence: list of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        log_prob = 0\n",
    "        sentence = tuple(sentence)\n",
    "        for i in range(2, len(sentence)):\n",
    "            # 注意这是对数运算的情况\n",
    "            # 这里i的取值会把<s>和</s>考虑进来，可以自行修改使得不考虑这两个词\n",
    "            # TODO: calculates the log probability\n",
    "            log_prob += math.log(self[sentence[max(1, i-self.n): i]])\n",
    "        return log_prob\n",
    "\n",
    "    def ppl(self, sentence: IntSentence) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the PPL of the given sentence. Assumes that the first token\n",
    "        is always \"<s>\".\n",
    "\n",
    "        Args:\n",
    "            sentence: list of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "        # 和我们课件中的PPW是同一个东西，利用上面的log_prob加以计算\n",
    "        # TODO: calculates the PPL\n",
    "        return 1 / pow(math.exp(self.log_prob(sentence)), 1 / (len(sentence) - 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbcf678",
   "metadata": {},
   "source": [
    "### 训练与测试\n",
    "\n",
    "现在数据与模型均已齐备，可以训练并测试了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd706656",
   "metadata": {},
   "source": [
    "训练模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5f988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped model.\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "model = NGramModel(len(vocabulary))\n",
    "model.learn(corpus)\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pkl.dump(vocabulary, f)\n",
    "    pkl.dump(model, f)\n",
    "\n",
    "print(\"Dumped model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f97ca6",
   "metadata": {},
   "source": [
    "在测试集上测试计算困惑度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf8b1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model.\n",
      "41758.445503209725\n",
      "819.1666149385499\n",
      "284.4607938559403\n",
      "81.01344582474758\n",
      "105650.84533930507\n",
      "64.02850793248672\n",
      "21.085407002643866\n",
      "185.41336489199156\n",
      "824.9038152916542\n",
      "70.74988919909318\n",
      "287.50507199565027\n",
      "109.88706089222921\n",
      "5149.674473527617\n",
      "1687.0411596550928\n",
      "385.4693705649921\n",
      "418.6058197700788\n",
      "327429.50704225356\n",
      "50615.642043398606\n",
      "86.81172590440617\n",
      "425.32242094930945\n",
      "706.2501807720853\n",
      "3916.798948946527\n",
      "335.280509450962\n",
      "326.60389244156363\n",
      "86.43227357675555\n",
      "73.25387975545632\n",
      "367.3440475763735\n",
      "27.97731576486108\n",
      "382.47065036736865\n",
      "215.76100722084104\n",
      "250.24494617899\n",
      "643.151773214768\n",
      "89.72478492963967\n",
      "169.35700001888958\n",
      "854.1236833136531\n",
      "84.0859437539743\n",
      "632.3826031094769\n",
      "517.796460985909\n",
      "336.2301241227322\n",
      "13234.038700198142\n",
      "156.45780994755333\n",
      "727.9335583367653\n",
      "122.98729435474569\n",
      "169.85986512441485\n",
      "1283.0432920781466\n",
      "1329.0107616248656\n",
      "307.3947456436026\n",
      "979.8574069785449\n",
      "191.16551949541588\n",
      "322.52075432451596\n",
      "Avg:  11303.90237207942\n"
     ]
    }
   ],
   "source": [
    "with open(\"model.pkl\", \"rb\") as f:\n",
    "    vocabulary = pkl.load(f)\n",
    "    model = pkl.load(f)\n",
    "print(\"Loaded model.\")\n",
    "\n",
    "with open(\"data/news.2007.en.shuffled.deduped.test\", encoding=\"utf-8\") as f:\n",
    "    test_set = list(map(lambda l: l.strip(), f.readlines()))\n",
    "test_corpus = normaltokenize(test_set)\n",
    "test_corpus = list(\n",
    "        map(functools.partial(words_to_indices, vocabulary),\n",
    "            test_corpus))\n",
    "ppls = []\n",
    "for t in test_corpus:\n",
    "    ppls.append(model.ppl(t))\n",
    "    print(ppls[-1])\n",
    "print(\"Avg: \", sum(ppls)/len(ppls))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "712078aab02776cde4a9f4a9afc393cea3823f2c3907018610aeec1eb331d661"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
